{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagify.config.config import ConfigManager\n",
    "import os\n",
    "\n",
    "config = ConfigManager(\".sagify.json\").get_config()\n",
    "role = os.getenv(\"ROLE_ARN\")\n",
    "bucket_uri = os.getenv(\"S3_BUCKET_URI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "source_path = Path(\"tmp/model\")\n",
    "target_path = Path(\"tmp/artifact/model.tar.gz\")\n",
    "\n",
    "with tarfile.open(target_path, \"w:gz\") as file:\n",
    "    for path in source_path.glob(\"*\"):\n",
    "        file.add(path, arcname=path.relative_to(source_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagify.sagemaker.sagemaker import SageMakerClient\n",
    "\n",
    "\n",
    "sage_maker_client = SageMakerClient(config.aws_profile, config.aws_region, role)\n",
    "\n",
    "input_dir = \"tmp/artifact\"\n",
    "s3_dir = \"inference/mle-on-aws-book\"\n",
    "sage_maker_client.upload_data(input_dir, s3_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sagify push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy serverless endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "def construct_image_location(image_name: str, boto_session: boto3.Session) -> str:\n",
    "    account = boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "    region = boto_session.region_name\n",
    "    return \"{account}.dkr.ecr.{region}.amazonaws.com/{image}\".format(\n",
    "        account=account, region=region, image=image_name\n",
    "    )\n",
    "\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(memory_size_in_mb=1024, max_concurrency=1)\n",
    "boto_session = boto3.Session(\n",
    "    region_name=config.aws_region, profile_name=config.aws_profile\n",
    ")\n",
    "image_uri = f\"{construct_image_location(config.image_name, boto_session)}:latest\"\n",
    "print(image_uri)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=f\"{bucket_uri}/inference/mle-on-aws-book/model.tar.gz\",\n",
    "    name=\"mle-on-aws-book\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "endpoint_name = \"mle-on-aws-book\"\n",
    "model.deploy(serverless_inference_config=serverless_config, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from sagemaker import Session\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "endpoint = \"mle-on-aws-book\"\n",
    "boto_session = boto3.Session(region_name=\"us-east-1\")\n",
    "sagemaker_session = Session(boto_session)\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "for _ in range(10):\n",
    "    print(predictor.predict({\"x\": random()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import requests\n",
    "\n",
    "input = random()\n",
    "response = requests.post(\"http://localhost:8080/invocations\", json={\"x\": input})\n",
    "print(response)\n",
    "response.json()\n",
    "print(f\"Input: {input}, Prediction: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def invoke(input):\n",
    "    response = requests.post(\"http://localhost:8080/invocations\", json={\"x\": input})\n",
    "    # print(f\"Input: {input}, Prediction: {response.json()}\")\n",
    "\n",
    "\n",
    "n = 1000\n",
    "with ThreadPool(10) as pool:\n",
    "    with tqdm(total=n) as pbar:\n",
    "        for _ in pool.imap_unordered(invoke, [random() for _ in range(n)]):\n",
    "            pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
